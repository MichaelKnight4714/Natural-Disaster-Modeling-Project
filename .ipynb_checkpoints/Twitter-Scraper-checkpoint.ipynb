{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import tweepy\n",
    "import datetime\n",
    "import json, codecs\n",
    "import jsonpickle\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter API credentials\n",
    "# Credit to Temple Moore for authentication help and setting up tweepy\n",
    "\n",
    "with open('./credentials.json') as creds:\n",
    "    info = json.load(creds)\n",
    "    consumer_key = info['CONSUMER_KEY']\n",
    "    consumer_secret = info['CONSUMER_SECRET']\n",
    "    access_token = info['ACCESS_TOKEN']\n",
    "    access_secret = info['ACCESS_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autentication Successful\n"
     ]
    }
   ],
   "source": [
    "# Authenticate to Twitter\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Autentication Successful\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC DSI Client Project - Problem 1\n",
      "{'/search/tweets': {'limit': 180, 'remaining': 180, 'reset': 1563911795}}\n"
     ]
    }
   ],
   "source": [
    "user = api.me()\n",
    "print(user.name)\n",
    "print(api.rate_limit_status()['resources']['search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GetOldTweets3.manager.TweetCriteria.TweetCriteria object at 0x11758b588>\n",
      "Shape of DataFrame: (2696, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macmedics/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code modified from Jasmine Vasandani via Temple Moore\n",
    "# With the help of got.manager, get all the tweets from 07/01/2019 till 07/08/2019\n",
    "# Store all the collected tweets in DF\n",
    "\n",
    "earthquake=[]\n",
    "final_df=pd.DataFrame()\n",
    "tweetCriteria = got.manager.TweetCriteria().setSince(\"2019-07-01\").setUntil(\"2019-07-08\").setQuerySearch('earthquake').setNear('Las Vegas').setWithin('25mi')\n",
    "print(tweetCriteria)\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "for i in range(len(tweet)):\n",
    "    tweet_dict = {}\n",
    "    tweet_dict['id'] = tweet[i].id\n",
    "    tweet_dict['username'] = tweet[i].username\n",
    "    tweet_dict['date'] = tweet[i].date\n",
    "    tweet_dict['text'] = tweet[i].text\n",
    "    tweet_dict['hashtags'] = tweet[i].hashtags\n",
    "    tweet_dict['geo'] = tweet[i].geo\n",
    "    tweet_dict['type'] = 'official'\n",
    "    earthquake.append(tweet_dict)\n",
    "\n",
    "df1=pd.DataFrame(earthquake)\n",
    "final_df = pd.concat([df1.reset_index(), final_df.reset_index()])\n",
    "print(\"Shape of DataFrame:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    11928\n",
       "Name: geo, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['geo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2696, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    206\n",
       "Name: geo, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['geo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves the dataframe to a csv - change the name for each individual scrape\n",
    "# df1.to_csv('./lasvegas_earthquake2019_25mi.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
